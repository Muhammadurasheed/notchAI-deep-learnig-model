{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5baa8c06-9f8c-48e7-8252-bef925458ec3",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"dermatology_icon.png\" alt=Dermatologyo\" title=Dermatologyo\" width=\"150\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927e310-79bf-4899-9171-7d0048116b20",
   "metadata": {},
   "source": [
    "<a id='Data-Exploration'></a>\n",
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352e30f4-8453-4978-b834-deb37d229423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and configurations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import datetime\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input, concatenate, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.applications.densenet import DenseNet201, DenseNet121\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, top_k_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a87dab2-caf5-4112-b3da-7862849f7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "dataset = pd.read_csv('fitzpatrick17k_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891b74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset containing the pictures without body hair\n",
    "dataset = dataset[dataset['image_path'].notna()]\n",
    "dataset['image_path'] = 'images_hair_removed/' + dataset['url_alphanum'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa834f-6757-4f3b-9308-6489c07ecf2a",
   "metadata": {},
   "source": [
    "<a id='Data-Preprocessing'></a>\n",
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e2ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the dataset for preprocessing purposes\n",
    "df_processed = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a52929-2621-4bf3-9b99-ca8f2ed7c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the records without a url, since the model won't be able to learn from records without pictures\n",
    "df_processed = df_processed[dataset['image_path'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83089696-7ad3-4c0a-a697-b699ee6df26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the irrelevant features for the deep learning model\n",
    "df_processed.drop(['md5hash', 'qc', 'url_alphanum', 'three_partition_label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d366c9e6-8510-4ec2-9809-6fb3d5213647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the records where both 'fitzpatrick_scale' and 'fitzpatrick_centaur' are -1\n",
    "df_processed = df_processed[~((df_processed['fitzpatrick_scale'] == -1) & (df_processed['fitzpatrick_centaur'] == -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90cce00-6f77-4461-8bf3-2855ba89073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate an average based on both fitzpatrick columns\n",
    "def calculate_fitzpatrick_average(row):\n",
    "    if row['fitzpatrick_scale'] == -1 and row['fitzpatrick_centaur'] != -1:\n",
    "        return row['fitzpatrick_centaur']\n",
    "    elif row['fitzpatrick_centaur'] == -1 and row['fitzpatrick_scale'] != -1:\n",
    "        return row['fitzpatrick_scale']\n",
    "    else:\n",
    "        return (row['fitzpatrick_scale'] + row['fitzpatrick_centaur']) / 2\n",
    "\n",
    "# Apply the function to each row\n",
    "df_processed['fitzpatrick_average'] = df_processed.apply(calculate_fitzpatrick_average, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c3302c-e2c4-4661-b53b-fc929a9c4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is now composed of 16236 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset is now composed of {df_processed.shape[0]} rows and {df_processed.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42509e6d-ef3a-4ef7-b222-85d57137875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fitzpatrick_scale</th>\n",
       "      <th>fitzpatrick_centaur</th>\n",
       "      <th>label</th>\n",
       "      <th>nine_partition_label</th>\n",
       "      <th>url</th>\n",
       "      <th>image_path</th>\n",
       "      <th>fitzpatrick_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>drug induced pigmentary changes</td>\n",
       "      <td>inflammatory</td>\n",
       "      <td>https://www.dermaamin.com/site/images/clinical...</td>\n",
       "      <td>images_hair_removed/httpwwwdermaamincomsiteima...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>photodermatoses</td>\n",
       "      <td>inflammatory</td>\n",
       "      <td>https://www.dermaamin.com/site/images/clinical...</td>\n",
       "      <td>images_hair_removed/httpwwwdermaamincomsiteima...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>dermatofibroma</td>\n",
       "      <td>benign dermal</td>\n",
       "      <td>https://www.dermaamin.com/site/images/clinical...</td>\n",
       "      <td>images_hair_removed/httpwwwdermaamincomsiteima...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fitzpatrick_scale  fitzpatrick_centaur                            label  \\\n",
       "0                  3                    3  drug induced pigmentary changes   \n",
       "1                  1                    1                  photodermatoses   \n",
       "2                  2                    3                   dermatofibroma   \n",
       "\n",
       "  nine_partition_label                                                url  \\\n",
       "0         inflammatory  https://www.dermaamin.com/site/images/clinical...   \n",
       "1         inflammatory  https://www.dermaamin.com/site/images/clinical...   \n",
       "2        benign dermal  https://www.dermaamin.com/site/images/clinical...   \n",
       "\n",
       "                                          image_path  fitzpatrick_average  \n",
       "0  images_hair_removed/httpwwwdermaamincomsiteima...                  3.0  \n",
       "1  images_hair_removed/httpwwwdermaamincomsiteima...                  1.0  \n",
       "2  images_hair_removed/httpwwwdermaamincomsiteima...                  2.5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the first 3 records of the dataset\n",
    "df_processed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974a5fd",
   "metadata": {},
   "source": [
    "<a id='RGB-Conversion'></a>\n",
    "### 2.1 Convert Images to an RGB array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307c4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path, target_size=(128, 128)):\n",
    "    try:\n",
    "        image = Image.open(path)                                # Open the image from the local path\n",
    "        image = image.convert('RGB')                            # Convert to RGB\n",
    "        image = np.array(image)                                 # Convert the image to a numpy array\n",
    "        image = tf.convert_to_tensor(image, dtype=tf.float16)   # Convert to TensorFlow tensor\n",
    "        image = tf.image.resize(image, target_size)             # Resize the image to the target size\n",
    "        image /= 255.0                                         # Normalize the image values to be between 0 and 1\n",
    "        image = np.expand_dims(image, axis=0)                   # Add a batch dimension\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Path: {path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c51867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column containing the RGB matrix of the pictures\n",
    "df_processed['rgb'] = df_processed['image_path'].apply(lambda x: load_and_preprocess_image(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34eb60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the images' data\n",
    "images = list()\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for i in range(df_processed.shape[0]):\n",
    "    # Append the image data (from 'rgb' column) of each row to the 'images' list\n",
    "    # 'iloc[i]' is used to access the i-th row of the dataframe.\n",
    "    images.append(df_processed['rgb'].iloc[i])\n",
    "\n",
    "# Convert the list of images into a NumPy array\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1044b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the array to fit the model input:\n",
    "# 'np.squeeze' is used to remove axes of length one from each sub-array within 'images'\n",
    "# This operation is done because the model will require input without these single dimensions\n",
    "# The loop iterates over each sub-array ('subarr') in 'images'\n",
    "reshaped_image = [np.squeeze(subarr) for subarr in images]\n",
    "\n",
    "# Convert the list of reshaped arrays back into a single NumPy array because the model expects the input to be a NumPy array rather than a list of arrays\n",
    "images = np.array(reshaped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c338fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16236, 128, 128, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the shape of the images array\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2a5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels:\n",
    "# Initialize a LabelEncoder instance that will be used to convert categorical text labels into a numeric format, making them readable for the models\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transform the labels into a numeric format:\n",
    "# 1. Extract the 'label' column from 'df_processed' dataframe and convert it to a NumPy array\n",
    "# 2. Use the 'fit_transform' method of LabelEncoder to fit the label encoder and return encoded labels\n",
    "#    This method first fits the label encoder to the data (learning the unique labels) and then transforms the labels to numeric values\n",
    "# The transformed labels are stored in the 'labels' variable\n",
    "labels = label_encoder.fit_transform(np.array(df_processed['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b45d0a-d392-4e62-a433-6932912dd13d",
   "metadata": {},
   "source": [
    "<a id='Train-validation-test-split'></a>\n",
    "### 2.2 Split the data into train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12962f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fitzpatrick_scale', 'fitzpatrick_centaur', 'label',\n",
       "       'nine_partition_label', 'url', 'image_path', 'fitzpatrick_average',\n",
       "       'rgb'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the current dataframe's columns\n",
    "df_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b116b8d8-4cb9-4774-b651-f507d324d3de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (9741, 1) (9741,)\n",
      "Validation set: (3247, 1) (3247,)\n",
      "Test set: (3248, 1) (3248,)\n"
     ]
    }
   ],
   "source": [
    "# Separate the dataset into numerical, categorical and images\n",
    "df_numerical = df_processed[['fitzpatrick_average']]\n",
    "df_categorical = df_processed[['nine_partition_label']]\n",
    "\n",
    "# First split: separate out the training data (60%), while preserving the distribution of label classes in the original dataset, ensuring representativity\n",
    "X_train_image, X_temp_image, X_train_numerical, X_temp_numerical, X_train_categorical, X_temp_categorical, y_train, y_temp = train_test_split(images, df_numerical, df_categorical, labels, test_size=0.4, stratify=labels, random_state=42)\n",
    "\n",
    "# Second split: divide the remaining 40% into validation and test sets (50% each of the remaining data), \n",
    "# while preserving the distribution of label classes in the original dataset, ensuring representativity\n",
    "X_val_image, X_test_image, X_val_numerical, X_test_numerical, X_val_categorical, X_test_categorical, y_val, y_test = train_test_split(X_temp_image, X_temp_numerical, X_temp_categorical, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# This results in 60% training, 20% validation, and 20% test\n",
    "print(\"Training set:\", X_train_numerical.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val_numerical.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test_numerical.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374fa0a-b006-4009-bc30-06791f661479",
   "metadata": {},
   "source": [
    "<a id='Normalization'></a>\n",
    "### 2.3 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "361a69f5-c559-4af5-a940-1b4926244e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler exclusively on the training data, to avoid Data Leakage\n",
    "scaler.fit(X_train_numerical)\n",
    "\n",
    "# Apply the transformation to the training, validation, and test data\n",
    "X_train_numerical = scaler.transform(X_train_numerical)\n",
    "X_val_numerical = scaler.transform(X_val_numerical)\n",
    "X_test_numerical = scaler.transform(X_test_numerical)\n",
    "\n",
    "# Now the 'fitzpatrick_average' column is scaled from 0 to 1 in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee1d1410-f977-4aff-99d4-dfc331029f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum fitzpatrick_average in train 0.0\n",
      "Maximum fitzpatrick_average in train 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Confirm the success of the normalization in the training dataset\n",
    "print(f'Minimum fitzpatrick_average in train {X_train_numerical.min()}')\n",
    "print(f'Maximum fitzpatrick_average in train {X_train_numerical.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ba8d293-4609-4b2c-a75a-628d90130cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum fitzpatrick_average in validation 0.0\n",
      "Maximum fitzpatrick_average in validation 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Confirm the success of the normalization in the validation dataset\n",
    "print(f'Minimum fitzpatrick_average in validation {X_val_numerical.min()}')\n",
    "print(f'Maximum fitzpatrick_average in validation {X_val_numerical.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c35c6e67-057a-4947-9b6c-f674d0680d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum fitzpatrick_average in test 0.0\n",
      "Maximum fitzpatrick_average in test 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Confirm the success of the normalization in the testing dataset\n",
    "print(f'Minimum fitzpatrick_average in test {X_test_numerical.min()}')\n",
    "print(f'Maximum fitzpatrick_average in test {X_test_numerical.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2243588",
   "metadata": {},
   "source": [
    "<a id='One-Hot-Encoding'></a>\n",
    "### 2.4 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8e5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OneHotEncoder\n",
    "# Set `handle_unknown='ignore` to ignore categories that weren't seen during `fit`\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder exclusively on the training data, to avoid Data Leakage\n",
    "encoder.fit(X_train_categorical)\n",
    "\n",
    "# Transform the training data\n",
    "train_encoded = encoder.transform(X_train_categorical)\n",
    "\n",
    "# Transform the validation and test data\n",
    "val_encoded = encoder.transform(X_val_categorical)\n",
    "test_encoded = encoder.transform(X_test_categorical)\n",
    "\n",
    "# The output of the encoding is a sparse matrix\n",
    "# Convert it back to a DataFrame\n",
    "X_train_one_hot = pd.DataFrame(train_encoded.toarray(), columns=encoder.get_feature_names_out())\n",
    "X_val_one_hot = pd.DataFrame(val_encoded.toarray(), columns=encoder.get_feature_names_out())\n",
    "X_test_one_hot = pd.DataFrame(test_encoded.toarray(), columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5076f5-68b3-4147-84b9-ec6d6addcaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nine_partition_label_benign dermal</th>\n",
       "      <th>nine_partition_label_benign epidermal</th>\n",
       "      <th>nine_partition_label_benign melanocyte</th>\n",
       "      <th>nine_partition_label_genodermatoses</th>\n",
       "      <th>nine_partition_label_inflammatory</th>\n",
       "      <th>nine_partition_label_malignant cutaneous lymphoma</th>\n",
       "      <th>nine_partition_label_malignant dermal</th>\n",
       "      <th>nine_partition_label_malignant epidermal</th>\n",
       "      <th>nine_partition_label_malignant melanoma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nine_partition_label_benign dermal  nine_partition_label_benign epidermal  \\\n",
       "0                                 0.0                                    0.0   \n",
       "1                                 0.0                                    0.0   \n",
       "2                                 0.0                                    0.0   \n",
       "\n",
       "   nine_partition_label_benign melanocyte  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "\n",
       "   nine_partition_label_genodermatoses  nine_partition_label_inflammatory  \\\n",
       "0                                  0.0                                1.0   \n",
       "1                                  0.0                                1.0   \n",
       "2                                  0.0                                1.0   \n",
       "\n",
       "   nine_partition_label_malignant cutaneous lymphoma  \\\n",
       "0                                                0.0   \n",
       "1                                                0.0   \n",
       "2                                                0.0   \n",
       "\n",
       "   nine_partition_label_malignant dermal  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "\n",
       "   nine_partition_label_malignant epidermal  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "\n",
       "   nine_partition_label_malignant melanoma  \n",
       "0                                      0.0  \n",
       "1                                      0.0  \n",
       "2                                      0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the success of the one-hot encoding procedure on the training dataset\n",
    "X_train_one_hot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251f3e6f-d99e-44f4-848a-3f3dc2fef879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nine_partition_label_benign dermal</th>\n",
       "      <th>nine_partition_label_benign epidermal</th>\n",
       "      <th>nine_partition_label_benign melanocyte</th>\n",
       "      <th>nine_partition_label_genodermatoses</th>\n",
       "      <th>nine_partition_label_inflammatory</th>\n",
       "      <th>nine_partition_label_malignant cutaneous lymphoma</th>\n",
       "      <th>nine_partition_label_malignant dermal</th>\n",
       "      <th>nine_partition_label_malignant epidermal</th>\n",
       "      <th>nine_partition_label_malignant melanoma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nine_partition_label_benign dermal  nine_partition_label_benign epidermal  \\\n",
       "0                                 0.0                                    0.0   \n",
       "1                                 0.0                                    0.0   \n",
       "2                                 0.0                                    0.0   \n",
       "\n",
       "   nine_partition_label_benign melanocyte  \\\n",
       "0                                     0.0   \n",
       "1                                     1.0   \n",
       "2                                     0.0   \n",
       "\n",
       "   nine_partition_label_genodermatoses  nine_partition_label_inflammatory  \\\n",
       "0                                  0.0                                1.0   \n",
       "1                                  0.0                                0.0   \n",
       "2                                  0.0                                1.0   \n",
       "\n",
       "   nine_partition_label_malignant cutaneous lymphoma  \\\n",
       "0                                                0.0   \n",
       "1                                                0.0   \n",
       "2                                                0.0   \n",
       "\n",
       "   nine_partition_label_malignant dermal  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "\n",
       "   nine_partition_label_malignant epidermal  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "\n",
       "   nine_partition_label_malignant melanoma  \n",
       "0                                      0.0  \n",
       "1                                      0.0  \n",
       "2                                      0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the success of the one-hot encoding procedure on the validation dataset\n",
    "X_val_one_hot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ee6418d-7393-470b-991d-d5336780b574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nine_partition_label_benign dermal</th>\n",
       "      <th>nine_partition_label_benign epidermal</th>\n",
       "      <th>nine_partition_label_benign melanocyte</th>\n",
       "      <th>nine_partition_label_genodermatoses</th>\n",
       "      <th>nine_partition_label_inflammatory</th>\n",
       "      <th>nine_partition_label_malignant cutaneous lymphoma</th>\n",
       "      <th>nine_partition_label_malignant dermal</th>\n",
       "      <th>nine_partition_label_malignant epidermal</th>\n",
       "      <th>nine_partition_label_malignant melanoma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nine_partition_label_benign dermal  nine_partition_label_benign epidermal  \\\n",
       "0                                 0.0                                    0.0   \n",
       "1                                 1.0                                    0.0   \n",
       "2                                 0.0                                    0.0   \n",
       "\n",
       "   nine_partition_label_benign melanocyte  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "\n",
       "   nine_partition_label_genodermatoses  nine_partition_label_inflammatory  \\\n",
       "0                                  0.0                                1.0   \n",
       "1                                  0.0                                0.0   \n",
       "2                                  0.0                                1.0   \n",
       "\n",
       "   nine_partition_label_malignant cutaneous lymphoma  \\\n",
       "0                                                0.0   \n",
       "1                                                0.0   \n",
       "2                                                0.0   \n",
       "\n",
       "   nine_partition_label_malignant dermal  \\\n",
       "0                                    0.0   \n",
       "1                                    0.0   \n",
       "2                                    0.0   \n",
       "\n",
       "   nine_partition_label_malignant epidermal  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "\n",
       "   nine_partition_label_malignant melanoma  \n",
       "0                                      0.0  \n",
       "1                                      0.0  \n",
       "2                                      0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm the success of the one-hot encoding procedure on the test dataset\n",
    "X_test_one_hot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8deab45-2ab6-451c-bdff-1fe65f0172ea",
   "metadata": {},
   "source": [
    "#### Check that every target class is present in all train, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d11d0f-f9ef-4914-a6ef-d90bfcb3fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label representativity is ensured across all datasets\n"
     ]
    }
   ],
   "source": [
    "# Define a function to check if all labels in validation and test sets are also present in the training set\n",
    "def check_label_representativity(train, val, test):\n",
    "\n",
    "    # Define unique labels in train, val and test\n",
    "    unique_train = np.unique(train)\n",
    "    unique_val = np.unique(val)\n",
    "    unique_test = np.unique(test)\n",
    "    \n",
    "    # Iterate through each unique label in the validation set\n",
    "    for label in unique_val:\n",
    "        # Check if the current label from the validation set is not in the training set\n",
    "        if label not in unique_train:\n",
    "            # If a label in the validation set is not found in the training set, raise an exception\n",
    "            raise Exception(f'Label {label} present in validation, but not in train')\n",
    "            \n",
    "    # Repeat the process for the test set\n",
    "    for label in unique_test:\n",
    "        # Check if the current label from the test set is not in the training set\n",
    "        if label not in unique_train:\n",
    "            # If a label in the test set is not found in the training set, raise an exception\n",
    "            raise Exception(f'Label {label} present in test, but not in train') \n",
    "            \n",
    "    # If no exceptions are raised, print a message indicating label representativity is ensured\n",
    "    print('Label representativity is ensured across all datasets')\n",
    "\n",
    "# Call the function with the labels of training, validation, and test sets as arguments\n",
    "check_label_representativity(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb382-57da-4919-a542-d6f966d6cbe2",
   "metadata": {},
   "source": [
    "<a id='Data-Augmentation'></a>\n",
    "### 2.5 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae9b98f-3b39-4b35-bcfe-4f3297788d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting class: 0\n",
      "Augmenting class: 1\n",
      "Augmenting class: 2\n",
      "Augmenting class: 3\n",
      "Augmenting class: 4\n",
      "Augmenting class: 5\n",
      "Augmenting class: 6\n",
      "Augmenting class: 7\n",
      "Augmenting class: 8\n",
      "Augmenting class: 9\n",
      "Augmenting class: 10\n",
      "Augmenting class: 11\n",
      "Augmenting class: 12\n",
      "Augmenting class: 13\n",
      "Augmenting class: 14\n",
      "Augmenting class: 15\n",
      "Augmenting class: 16\n",
      "Augmenting class: 17\n",
      "Augmenting class: 18\n",
      "Augmenting class: 19\n",
      "Augmenting class: 20\n",
      "Augmenting class: 21\n",
      "Augmenting class: 22\n",
      "Augmenting class: 23\n",
      "Augmenting class: 24\n",
      "Augmenting class: 25\n",
      "Augmenting class: 26\n",
      "Augmenting class: 27\n",
      "Augmenting class: 28\n",
      "Augmenting class: 29\n",
      "Augmenting class: 30\n",
      "Augmenting class: 31\n",
      "Augmenting class: 32\n",
      "Augmenting class: 33\n",
      "Augmenting class: 34\n",
      "Augmenting class: 35\n",
      "Augmenting class: 36\n",
      "Augmenting class: 37\n",
      "Augmenting class: 38\n",
      "Augmenting class: 39\n",
      "Augmenting class: 40\n",
      "Augmenting class: 41\n",
      "Augmenting class: 42\n",
      "Augmenting class: 43\n",
      "Augmenting class: 44\n",
      "Augmenting class: 45\n",
      "Augmenting class: 46\n",
      "Augmenting class: 47\n",
      "Augmenting class: 48\n",
      "Augmenting class: 49\n",
      "Augmenting class: 50\n",
      "Augmenting class: 51\n",
      "Augmenting class: 52\n",
      "Augmenting class: 53\n",
      "Augmenting class: 54\n",
      "Augmenting class: 55\n",
      "Augmenting class: 56\n",
      "Augmenting class: 57\n",
      "Augmenting class: 58\n",
      "Augmenting class: 59\n",
      "Augmenting class: 60\n",
      "Augmenting class: 61\n",
      "Augmenting class: 62\n",
      "Augmenting class: 63\n",
      "Augmenting class: 64\n",
      "Augmenting class: 65\n",
      "Augmenting class: 66\n",
      "Augmenting class: 67\n",
      "Augmenting class: 68\n",
      "Augmenting class: 69\n",
      "Augmenting class: 70\n",
      "Augmenting class: 71\n",
      "Augmenting class: 72\n",
      "Augmenting class: 73\n",
      "Augmenting class: 74\n",
      "Augmenting class: 75\n",
      "Augmenting class: 76\n",
      "Augmenting class: 77\n",
      "Augmenting class: 78\n",
      "Augmenting class: 79\n",
      "Augmenting class: 80\n",
      "Augmenting class: 81\n",
      "Augmenting class: 82\n",
      "Augmenting class: 83\n",
      "Augmenting class: 84\n",
      "Augmenting class: 85\n",
      "Augmenting class: 87\n",
      "Augmenting class: 88\n",
      "Augmenting class: 89\n",
      "Augmenting class: 90\n",
      "Augmenting class: 91\n",
      "Augmenting class: 92\n",
      "Augmenting class: 93\n",
      "Augmenting class: 94\n",
      "Augmenting class: 95\n",
      "Augmenting class: 96\n",
      "Augmenting class: 97\n",
      "Augmenting class: 99\n",
      "Augmenting class: 100\n",
      "Augmenting class: 101\n",
      "Augmenting class: 102\n",
      "Augmenting class: 103\n",
      "Augmenting class: 104\n",
      "Augmenting class: 105\n",
      "Augmenting class: 106\n",
      "Augmenting class: 107\n",
      "Augmenting class: 108\n",
      "Augmenting class: 109\n",
      "Augmenting class: 110\n",
      "Augmenting class: 111\n",
      "Augmenting class: 112\n",
      "Augmenting class: 113\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation setup\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2 # Zoom in by 20% maximum (randomly zoom images up to 20%)\n",
    ")\n",
    "\n",
    "# Initialize empty lists to store augmented data\n",
    "augmented_images = []\n",
    "augmented_numerical = [] \n",
    "augmented_one_hot = []  \n",
    "augmented_labels = []\n",
    "\n",
    "# Determine the number of samples to aim for each class after augmentation, to achieve balance\n",
    "target_samples_per_class = int(len(y_train) * 0.03)\n",
    "\n",
    "# Calculate how many samples each class needs based on the target\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_needs = {class_label: max(0, target_samples_per_class - count) for class_label, count in zip(unique, counts)}\n",
    "\n",
    "# Perform augmentation\n",
    "for class_label, needed_samples in class_needs.items():\n",
    "    if needed_samples <= 0:\n",
    "        continue  # Skip classes that don't need augmentation\n",
    "\n",
    "    # Indices of the current class in the dataset\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    \n",
    "    print(f'Augmenting class: {class_label}')\n",
    "\n",
    "    # Augment data for this class\n",
    "    for _ in range(needed_samples):\n",
    "        idx = np.random.choice(class_indices)\n",
    "        image_to_augment = X_train_image[idx].reshape((1,) + X_train_image[idx].shape)\n",
    "        one_hot = X_train_one_hot.iloc[idx].values  # Corresponding one-hot features\n",
    "        numerical = X_train_numerical[idx]          # Corresponding numerical feature\n",
    "        # Perform augmentation\n",
    "        it = data_augmentation.flow(image_to_augment, batch_size=1)\n",
    "        augmented_image = next(it)[0]\n",
    "\n",
    "        # Append the augmented data\n",
    "        augmented_images.append(augmented_image)\n",
    "        augmented_numerical.append(numerical)\n",
    "        augmented_one_hot.append(one_hot)\n",
    "        augmented_labels.append(class_label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_one_hot = np.array(augmented_one_hot)\n",
    "augmented_numerical = np.array(augmented_numerical)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "X_train_image_augmented = np.concatenate((X_train_image, augmented_images))\n",
    "X_train_one_hot_augmented = np.concatenate((X_train_one_hot, augmented_one_hot), axis=0)\n",
    "X_train_numerical_augmented = np.concatenate((X_train_numerical, augmented_numerical), axis=0)\n",
    "y_train_augmented = np.concatenate((y_train, augmented_labels))\n",
    "\n",
    "# Shuffle the augmented dataset\n",
    "shuffle_indices = np.random.permutation(len(X_train_image_augmented))\n",
    "X_train_image_augmented = X_train_image_augmented[shuffle_indices]\n",
    "X_train_numerical_augmented = X_train_numerical_augmented[shuffle_indices]\n",
    "X_train_one_hot_augmented = X_train_one_hot_augmented[shuffle_indices]\n",
    "y_train_augmented = y_train_augmented[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d0c1f58-6c63-4cd3-a07a-5f3e7d2acfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33420, 128, 128, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the shape of the augmented training dataset\n",
    "X_train_image_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb97c7b0-936f-4b71-89be-4c248d87d8ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution After Data Augmentation:\n",
      " 86     0.011550\n",
      "98     0.009874\n",
      "77     0.008737\n",
      "7      0.008737\n",
      "47     0.008737\n",
      "93     0.008737\n",
      "113    0.008737\n",
      "21     0.008737\n",
      "65     0.008737\n",
      "11     0.008737\n",
      "106    0.008737\n",
      "52     0.008737\n",
      "39     0.008737\n",
      "0      0.008737\n",
      "30     0.008737\n",
      "78     0.008737\n",
      "69     0.008737\n",
      "107    0.008737\n",
      "46     0.008737\n",
      "26     0.008737\n",
      "82     0.008737\n",
      "53     0.008737\n",
      "50     0.008737\n",
      "5      0.008737\n",
      "62     0.008737\n",
      "67     0.008737\n",
      "103    0.008737\n",
      "54     0.008737\n",
      "27     0.008737\n",
      "48     0.008737\n",
      "36     0.008737\n",
      "90     0.008737\n",
      "81     0.008737\n",
      "34     0.008737\n",
      "101    0.008737\n",
      "57     0.008737\n",
      "1      0.008737\n",
      "72     0.008737\n",
      "105    0.008737\n",
      "63     0.008737\n",
      "24     0.008737\n",
      "44     0.008737\n",
      "95     0.008737\n",
      "91     0.008737\n",
      "41     0.008737\n",
      "9      0.008737\n",
      "35     0.008737\n",
      "42     0.008737\n",
      "49     0.008737\n",
      "58     0.008737\n",
      "22     0.008737\n",
      "66     0.008737\n",
      "29     0.008737\n",
      "104    0.008737\n",
      "64     0.008737\n",
      "10     0.008737\n",
      "96     0.008737\n",
      "43     0.008737\n",
      "75     0.008737\n",
      "15     0.008737\n",
      "102    0.008737\n",
      "23     0.008737\n",
      "112    0.008737\n",
      "111    0.008737\n",
      "2      0.008737\n",
      "28     0.008737\n",
      "18     0.008737\n",
      "40     0.008737\n",
      "19     0.008737\n",
      "6      0.008737\n",
      "70     0.008737\n",
      "17     0.008737\n",
      "97     0.008737\n",
      "74     0.008737\n",
      "79     0.008737\n",
      "45     0.008737\n",
      "89     0.008737\n",
      "73     0.008737\n",
      "16     0.008737\n",
      "12     0.008737\n",
      "32     0.008737\n",
      "100    0.008737\n",
      "56     0.008737\n",
      "94     0.008737\n",
      "13     0.008737\n",
      "8      0.008737\n",
      "88     0.008737\n",
      "99     0.008737\n",
      "84     0.008737\n",
      "109    0.008737\n",
      "59     0.008737\n",
      "76     0.008737\n",
      "38     0.008737\n",
      "110    0.008737\n",
      "20     0.008737\n",
      "37     0.008737\n",
      "71     0.008737\n",
      "61     0.008737\n",
      "80     0.008737\n",
      "14     0.008737\n",
      "87     0.008737\n",
      "3      0.008737\n",
      "55     0.008737\n",
      "25     0.008737\n",
      "92     0.008737\n",
      "4      0.008737\n",
      "33     0.008737\n",
      "31     0.008737\n",
      "83     0.008737\n",
      "68     0.008737\n",
      "85     0.008737\n",
      "51     0.008737\n",
      "108    0.008737\n",
      "60     0.008737\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_series = pd.Series(y_train_augmented)  # Convert the labels into a Pandas Series\n",
    "\n",
    "# Temporarily adjust Pandas settings to display all rows since the number of unique labels is relatively big (114)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Get the distribution of labels, sort by values in descending order to see the proportions\n",
    "label_distribution = label_series.value_counts(normalize=True).sort_values(ascending=False)\n",
    "\n",
    "print(\"Label Distribution After Data Augmentation:\\n\", label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cecb4-ab3d-4571-a3ab-081a2dc36751",
   "metadata": {},
   "source": [
    "<a id='Modelling'></a>\n",
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4025289f-4215-4b7f-a455-e04a63cfaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete extra datasets to save memory\n",
    "del X_temp_image\n",
    "del X_temp_numerical\n",
    "del X_temp_categorical\n",
    "del y_temp\n",
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93565115-e5ad-47d3-b0f5-44688090f8a9",
   "metadata": {},
   "source": [
    "<a id='Base-Model'></a>\n",
    "### 3.1 Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd69f95b-a127-464b-b0f8-803f2cf2177f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 21s 69ms/step - loss: 4.5623 - accuracy: 0.0437 - val_loss: 4.4403 - val_accuracy: 0.0484 - lr: 0.0058\n",
      "Epoch 2/20\n",
      "305/305 [==============================] - 22s 71ms/step - loss: 4.3607 - accuracy: 0.0577 - val_loss: 4.3919 - val_accuracy: 0.0530 - lr: 0.0058\n",
      "Epoch 3/20\n",
      "305/305 [==============================] - 24s 79ms/step - loss: 4.2432 - accuracy: 0.0713 - val_loss: 4.3848 - val_accuracy: 0.0443 - lr: 0.0058\n",
      "Epoch 4/20\n",
      "305/305 [==============================] - 24s 77ms/step - loss: 4.1200 - accuracy: 0.0917 - val_loss: 4.2798 - val_accuracy: 0.0619 - lr: 0.0058\n",
      "Epoch 5/20\n",
      "305/305 [==============================] - 23s 75ms/step - loss: 3.9902 - accuracy: 0.1084 - val_loss: 4.2969 - val_accuracy: 0.0674 - lr: 0.0058\n",
      "Epoch 6/20\n",
      "305/305 [==============================] - 22s 72ms/step - loss: 3.8725 - accuracy: 0.1279 - val_loss: 4.2575 - val_accuracy: 0.0656 - lr: 0.0058\n",
      "Epoch 7/20\n",
      "305/305 [==============================] - 24s 80ms/step - loss: 3.7505 - accuracy: 0.1493 - val_loss: 4.2779 - val_accuracy: 0.0705 - lr: 0.0058\n",
      "Epoch 8/20\n",
      "305/305 [==============================] - 24s 79ms/step - loss: 3.6318 - accuracy: 0.1747 - val_loss: 4.2179 - val_accuracy: 0.0838 - lr: 0.0058\n",
      "Epoch 9/20\n",
      "305/305 [==============================] - 24s 79ms/step - loss: 3.5195 - accuracy: 0.1953 - val_loss: 4.1905 - val_accuracy: 0.0998 - lr: 0.0058\n",
      "Epoch 10/20\n",
      "305/305 [==============================] - 23s 75ms/step - loss: 3.3966 - accuracy: 0.2231 - val_loss: 4.2220 - val_accuracy: 0.0899 - lr: 0.0058\n",
      "Epoch 11/20\n",
      "305/305 [==============================] - 23s 74ms/step - loss: 3.2819 - accuracy: 0.2494 - val_loss: 4.2677 - val_accuracy: 0.0847 - lr: 0.0058\n",
      "Epoch 12/20\n",
      "305/305 [==============================] - ETA: 0s - loss: 3.1673 - accuracy: 0.2731\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005832082126289607.\n",
      "305/305 [==============================] - 23s 76ms/step - loss: 3.1673 - accuracy: 0.2731 - val_loss: 4.2618 - val_accuracy: 0.0884 - lr: 0.0058\n",
      "Epoch 13/20\n",
      "305/305 [==============================] - 26s 84ms/step - loss: 2.9297 - accuracy: 0.3578 - val_loss: 4.1304 - val_accuracy: 0.1133 - lr: 5.8321e-04\n",
      "Epoch 14/20\n",
      "305/305 [==============================] - 23s 77ms/step - loss: 2.8841 - accuracy: 0.3697 - val_loss: 4.1369 - val_accuracy: 0.1158 - lr: 5.8321e-04\n",
      "Epoch 15/20\n",
      "305/305 [==============================] - 24s 78ms/step - loss: 2.8643 - accuracy: 0.3742 - val_loss: 4.1449 - val_accuracy: 0.1090 - lr: 5.8321e-04\n",
      "Epoch 16/20\n",
      "304/305 [============================>.] - ETA: 0s - loss: 2.8484 - accuracy: 0.3751\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "305/305 [==============================] - 24s 79ms/step - loss: 2.8480 - accuracy: 0.3752 - val_loss: 4.1470 - val_accuracy: 0.1103 - lr: 5.8321e-04\n",
      "Epoch 17/20\n",
      "305/305 [==============================] - 25s 81ms/step - loss: 2.8171 - accuracy: 0.3809 - val_loss: 4.1411 - val_accuracy: 0.1133 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "305/305 [==============================] - 25s 83ms/step - loss: 2.8120 - accuracy: 0.3893 - val_loss: 4.1413 - val_accuracy: 0.1152 - lr: 1.0000e-04\n",
      "102/102 [==============================] - 2s 24ms/step\n",
      "Accuracy: [4.172484397888184, 0.10621920973062515]\n",
      "Test Accuracy: 0.1062192118226601\n",
      "F1 Score: 0.08619761448643103\n"
     ]
    }
   ],
   "source": [
    "def create_base_model():\n",
    "    \n",
    "\n",
    "    # Base model architecture\n",
    "    model = Sequential([\n",
    "        Conv2D(10, kernel_size=3, activation='relu', input_shape=(128, 128, 3)), \n",
    "        Flatten(),\n",
    "        Dense(114, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adagrad(learning_rate=0.00583208197094374)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_base_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x=X_train_image, y=y_train,\n",
    "    validation_data=(X_val_image, y_val),\n",
    "    epochs=20, verbose=1,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "accuracy = model.evaluate(X_test_image, y_test, verbose=0)\n",
    "\n",
    "scores = model.predict(X_test_image)\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22241d19-5264-4a92-aadc-7f2c939657b6",
   "metadata": {},
   "source": [
    "<a id='Regularized-Model'></a>\n",
    "### 3.2 Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27743685-356c-4357-9e60-ff9fc049533d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adagrad` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adagrad`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adagrad`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:42:51.937898: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 81s 525ms/step - loss: 4.9315 - accuracy: 0.0319 - val_loss: 5.1737 - val_accuracy: 0.0182 - lr: 0.0049\n",
      "Epoch 2/20\n",
      "153/153 [==============================] - 84s 550ms/step - loss: 4.4438 - accuracy: 0.0675 - val_loss: 4.8247 - val_accuracy: 0.0345 - lr: 0.0049\n",
      "Epoch 3/20\n",
      "153/153 [==============================] - 86s 561ms/step - loss: 4.1327 - accuracy: 0.1045 - val_loss: 4.5139 - val_accuracy: 0.0511 - lr: 0.0049\n",
      "Epoch 4/20\n",
      "153/153 [==============================] - 87s 568ms/step - loss: 3.8055 - accuracy: 0.1530 - val_loss: 4.4879 - val_accuracy: 0.0758 - lr: 0.0049\n",
      "Epoch 5/20\n",
      "153/153 [==============================] - 88s 576ms/step - loss: 3.4199 - accuracy: 0.2140 - val_loss: 4.1798 - val_accuracy: 0.1026 - lr: 0.0049\n",
      "Epoch 6/20\n",
      "153/153 [==============================] - 88s 578ms/step - loss: 2.9899 - accuracy: 0.2936 - val_loss: 4.1391 - val_accuracy: 0.1170 - lr: 0.0049\n",
      "Epoch 7/20\n",
      "153/153 [==============================] - 90s 591ms/step - loss: 2.5351 - accuracy: 0.3800 - val_loss: 4.1750 - val_accuracy: 0.1146 - lr: 0.0049\n",
      "Epoch 8/20\n",
      "153/153 [==============================] - 91s 593ms/step - loss: 2.0876 - accuracy: 0.4795 - val_loss: 4.3527 - val_accuracy: 0.1084 - lr: 0.0049\n",
      "Epoch 9/20\n",
      "153/153 [==============================] - ETA: 0s - loss: 1.6544 - accuracy: 0.5900\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0004901409614831209.\n",
      "153/153 [==============================] - 90s 591ms/step - loss: 1.6544 - accuracy: 0.5900 - val_loss: 4.2244 - val_accuracy: 0.1361 - lr: 0.0049\n",
      "Epoch 10/20\n",
      "153/153 [==============================] - 90s 586ms/step - loss: 1.1839 - accuracy: 0.7197 - val_loss: 4.0722 - val_accuracy: 0.1528 - lr: 4.9014e-04\n",
      "Epoch 11/20\n",
      "153/153 [==============================] - 89s 584ms/step - loss: 1.0121 - accuracy: 0.7700 - val_loss: 4.0797 - val_accuracy: 0.1543 - lr: 4.9014e-04\n",
      "Epoch 12/20\n",
      "153/153 [==============================] - 90s 588ms/step - loss: 0.9652 - accuracy: 0.7824 - val_loss: 4.0895 - val_accuracy: 0.1583 - lr: 4.9014e-04\n",
      "Epoch 13/20\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.9216 - accuracy: 0.7949\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "153/153 [==============================] - 89s 584ms/step - loss: 0.9216 - accuracy: 0.7949 - val_loss: 4.1243 - val_accuracy: 0.1543 - lr: 4.9014e-04\n",
      "Epoch 14/20\n",
      "153/153 [==============================] - 91s 593ms/step - loss: 0.8601 - accuracy: 0.8092 - val_loss: 4.0892 - val_accuracy: 0.1598 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "153/153 [==============================] - 89s 584ms/step - loss: 0.8524 - accuracy: 0.8133 - val_loss: 4.0833 - val_accuracy: 0.1605 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "\n",
    "    # Regularized model architecture\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(114, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adagrad(learning_rate=0.004901409801938672)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x=X_train_image, y=y_train,\n",
    "    validation_data=(X_val_image, y_val),\n",
    "    epochs=20, verbose=1,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4805b00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 7s 70ms/step\n",
      "Accuracy: [4.0721893310546875, 0.15275639295578003]\n",
      "Test Accuracy: 0.16410098522167488\n",
      "F1 Score: 0.15003962720098593\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "accuracy = model.evaluate(X_val_image, y_val, verbose=0)\n",
    "scores = model.predict(X_test_image)\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824d9b6-ec3f-43d8-8950-a0c6b3b6275a",
   "metadata": {},
   "source": [
    "<a id='Model-With-Functional-API-Augmented'></a>\n",
    "### 3.4 Model With Functional API With Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "870d90ab-a120-4d43-bec2-9b3ac23727b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "523/523 [==============================] - 318s 606ms/step - loss: 4.2676 - accuracy: 0.1125 - val_loss: 3.3266 - val_accuracy: 0.1518 - lr: 0.0043\n",
      "Epoch 2/20\n",
      "523/523 [==============================] - 319s 609ms/step - loss: 2.9674 - accuracy: 0.2320 - val_loss: 3.2526 - val_accuracy: 0.1534 - lr: 0.0043\n",
      "Epoch 3/20\n",
      "523/523 [==============================] - 308s 590ms/step - loss: 2.7182 - accuracy: 0.2900 - val_loss: 3.2005 - val_accuracy: 0.1931 - lr: 0.0043\n",
      "Epoch 4/20\n",
      "523/523 [==============================] - 311s 595ms/step - loss: 2.5111 - accuracy: 0.3360 - val_loss: 3.4055 - val_accuracy: 0.1552 - lr: 0.0043\n",
      "Epoch 5/20\n",
      "523/523 [==============================] - 308s 590ms/step - loss: 2.2977 - accuracy: 0.3911 - val_loss: 3.2508 - val_accuracy: 0.2026 - lr: 0.0043\n",
      "Epoch 6/20\n",
      "523/523 [==============================] - 312s 596ms/step - loss: 2.0738 - accuracy: 0.4485 - val_loss: 2.9493 - val_accuracy: 0.2796 - lr: 0.0043\n",
      "Epoch 7/20\n",
      "523/523 [==============================] - 311s 594ms/step - loss: 1.8670 - accuracy: 0.4972 - val_loss: 2.8111 - val_accuracy: 0.2836 - lr: 0.0043\n",
      "Epoch 8/20\n",
      "523/523 [==============================] - 316s 604ms/step - loss: 1.6861 - accuracy: 0.5425 - val_loss: 3.2662 - val_accuracy: 0.2393 - lr: 0.0043\n",
      "Epoch 9/20\n",
      "523/523 [==============================] - 896s 2s/step - loss: 1.5065 - accuracy: 0.5881 - val_loss: 2.8497 - val_accuracy: 0.2821 - lr: 0.0043\n",
      "Epoch 10/20\n",
      "523/523 [==============================] - ETA: 0s - loss: 1.3675 - accuracy: 0.6197\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0004253738094121218.\n",
      "523/523 [==============================] - 291s 557ms/step - loss: 1.3675 - accuracy: 0.6197 - val_loss: 2.9530 - val_accuracy: 0.2756 - lr: 0.0043\n",
      "Epoch 11/20\n",
      "523/523 [==============================] - 297s 568ms/step - loss: 1.0760 - accuracy: 0.6953 - val_loss: 2.8276 - val_accuracy: 0.3157 - lr: 4.2537e-04\n",
      "Epoch 12/20\n",
      "523/523 [==============================] - 331s 633ms/step - loss: 0.9750 - accuracy: 0.7220 - val_loss: 2.7789 - val_accuracy: 0.3258 - lr: 4.2537e-04\n",
      "Epoch 13/20\n",
      "523/523 [==============================] - 1625s 3s/step - loss: 0.9211 - accuracy: 0.7384 - val_loss: 2.8598 - val_accuracy: 0.3277 - lr: 4.2537e-04\n",
      "Epoch 14/20\n",
      "523/523 [==============================] - 282s 538ms/step - loss: 0.8652 - accuracy: 0.7526 - val_loss: 2.8439 - val_accuracy: 0.3354 - lr: 4.2537e-04\n",
      "Epoch 15/20\n",
      "523/523 [==============================] - ETA: 0s - loss: 0.8178 - accuracy: 0.7672\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "523/523 [==============================] - 289s 553ms/step - loss: 0.8178 - accuracy: 0.7672 - val_loss: 2.8514 - val_accuracy: 0.3382 - lr: 4.2537e-04\n",
      "Epoch 16/20\n",
      "523/523 [==============================] - 296s 566ms/step - loss: 0.7808 - accuracy: 0.7754 - val_loss: 2.8292 - val_accuracy: 0.3459 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "523/523 [==============================] - 315s 602ms/step - loss: 0.7755 - accuracy: 0.7778 - val_loss: 2.8337 - val_accuracy: 0.3446 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    \n",
    "    # Inputs\n",
    "    image_input = Input(shape=(128, 128, 3), name='image_input')\n",
    "    numerical_input = Input(shape=(1,), name='numerical_input')\n",
    "    one_hot_input = Input(shape=(9,), name='one_hot_input')\n",
    "\n",
    "    # Image processing branch\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.33846602232096573)(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.33846602232096573)(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.33846602232096573)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Numerical data processing branch\n",
    "    y = Dense(64, activation='relu')(numerical_input)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # One-hot encoded data processing branch\n",
    "    z = Dense(64, activation='relu')(one_hot_input)\n",
    "    z = BatchNormalization()(z)\n",
    "\n",
    "    # Concatenate all layers\n",
    "    combined = concatenate([x, y, z])\n",
    "    combined = Dense(512, activation='relu')(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = Dropout(0.668798647763459)(combined)\n",
    "    output = Dense(114, activation='softmax')(combined)\n",
    "\n",
    "        \n",
    "    optimizer = Adam(learning_rate=0.004253738267116145)\n",
    "\n",
    "    model = Model(inputs=[image_input, numerical_input, one_hot_input], outputs=output)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [X_train_image_augmented, X_train_numerical_augmented, X_train_one_hot_augmented], y_train_augmented,\n",
    "    validation_data=([X_val_image, X_val_numerical, X_val_one_hot], y_val),\n",
    "    epochs=20, verbose=1,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb2a13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 7s 70ms/step\n",
      "Accuracy: [2.812701940536499, 0.3158867061138153]\n",
      "Test Accuracy: 0.3158866995073892\n",
      "F1 Score: 0.3135323235002952\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.evaluate([X_test_image, X_test_numerical, X_test_one_hot], y_test, verbose=0)\n",
    "scores = model.predict([X_test_image, X_test_numerical, X_test_one_hot])\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b8c2d-3aa2-42a3-999a-05d1173a3853",
   "metadata": {},
   "source": [
    "<a id='Transfer-Learning-With-Functional-API'></a>\n",
    "### 3.6 Transfer Learning With Functional API With Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c0cc6-7c9c-4833-a76e-40c0c6a6565e",
   "metadata": {},
   "source": [
    "#### **Dense Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aaad6256-3c53-4523-bf46-892b809437db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "523/523 [==============================] - 629s 1s/step - loss: 5.0447 - accuracy: 0.0750 - val_loss: 3.5003 - val_accuracy: 0.2147 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "523/523 [==============================] - 641s 1s/step - loss: 3.4901 - accuracy: 0.2228 - val_loss: 2.8702 - val_accuracy: 0.3107 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "523/523 [==============================] - 647s 1s/step - loss: 2.8201 - accuracy: 0.3185 - val_loss: 2.6291 - val_accuracy: 0.3486 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "523/523 [==============================] - 675s 1s/step - loss: 2.4489 - accuracy: 0.3876 - val_loss: 2.4982 - val_accuracy: 0.3699 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "523/523 [==============================] - 693s 1s/step - loss: 2.1969 - accuracy: 0.4355 - val_loss: 2.4153 - val_accuracy: 0.3847 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "523/523 [==============================] - 722s 1s/step - loss: 2.0023 - accuracy: 0.4738 - val_loss: 2.3558 - val_accuracy: 0.3936 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "523/523 [==============================] - 686s 1s/step - loss: 1.8370 - accuracy: 0.5109 - val_loss: 2.3263 - val_accuracy: 0.3961 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "523/523 [==============================] - 711s 1s/step - loss: 1.6859 - accuracy: 0.5480 - val_loss: 2.2852 - val_accuracy: 0.4025 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "523/523 [==============================] - 733s 1s/step - loss: 1.5524 - accuracy: 0.5812 - val_loss: 2.2793 - val_accuracy: 0.4084 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "523/523 [==============================] - 709s 1s/step - loss: 1.4299 - accuracy: 0.6095 - val_loss: 2.2533 - val_accuracy: 0.4173 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "523/523 [==============================] - 731s 1s/step - loss: 1.3241 - accuracy: 0.6348 - val_loss: 2.2932 - val_accuracy: 0.4179 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "523/523 [==============================] - 718s 1s/step - loss: 1.2223 - accuracy: 0.6610 - val_loss: 2.2483 - val_accuracy: 0.4256 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "523/523 [==============================] - 723s 1s/step - loss: 1.1297 - accuracy: 0.6857 - val_loss: 2.2661 - val_accuracy: 0.4232 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "523/523 [==============================] - 733s 1s/step - loss: 1.0461 - accuracy: 0.7063 - val_loss: 2.2491 - val_accuracy: 0.4339 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "523/523 [==============================] - ETA: 0s - loss: 0.9627 - accuracy: 0.7282\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "523/523 [==============================] - 712s 1s/step - loss: 0.9627 - accuracy: 0.7282 - val_loss: 2.2931 - val_accuracy: 0.4272 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "523/523 [==============================] - 722s 1s/step - loss: 0.8554 - accuracy: 0.7573 - val_loss: 2.2219 - val_accuracy: 0.4398 - lr: 1.0000e-03\n",
      "Epoch 17/20\n",
      "523/523 [==============================] - 731s 1s/step - loss: 0.8195 - accuracy: 0.7704 - val_loss: 2.2162 - val_accuracy: 0.4490 - lr: 1.0000e-03\n",
      "Epoch 18/20\n",
      "523/523 [==============================] - 721s 1s/step - loss: 0.7941 - accuracy: 0.7754 - val_loss: 2.2155 - val_accuracy: 0.4447 - lr: 1.0000e-03\n",
      "Epoch 19/20\n",
      "523/523 [==============================] - 733s 1s/step - loss: 0.7841 - accuracy: 0.7802 - val_loss: 2.2202 - val_accuracy: 0.4460 - lr: 1.0000e-03\n",
      "Epoch 20/20\n",
      "523/523 [==============================] - 730s 1s/step - loss: 0.7645 - accuracy: 0.7829 - val_loss: 2.2254 - val_accuracy: 0.4506 - lr: 1.0000e-03\n"
     ]
    }
   ],
   "source": [
    "def create_model(dropout_rate, num_units, num_units_one_hot, optimizer, lr, dropout_rate_combined, loss, metrics):\n",
    "    \n",
    "    # Inputs\n",
    "    image_input = Input(shape=(128, 128, 3), name='image_input')\n",
    "    numerical_input = Input(shape=(1,), name='numerical_input')\n",
    "    one_hot_input = Input(shape=(9,), name='one_hot_input')\n",
    "\n",
    "    # DenseNet201-based image processing branch\n",
    "    base_model = DenseNet201(include_top=False, weights=\"imagenet\", input_tensor=image_input)\n",
    "\n",
    "    # Only train the last few layers (specifically, layers after 'conv5_block30_0_bn')\n",
    "    flag = False\n",
    "    for layer in base_model.layers:\n",
    "        if layer.name == 'conv5_block30_0_bn' or flag:\n",
    "            layer.trainable = True\n",
    "            flag = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Numerical data processing branch\n",
    "    y = Dense(num_units)(numerical_input)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # One-hot encoded data processing branch\n",
    "    z = Dense(num_units_one_hot)(one_hot_input)\n",
    "    z = BatchNormalization()(z)\n",
    "\n",
    "    # Concatenate all layers\n",
    "    combined = concatenate([x, y, z])\n",
    "    combined = Dense(512, activation='relu')(combined)\n",
    "    combined = BatchNormalization()(combined)\n",
    "    combined = Dropout(dropout_rate_combined)(combined)\n",
    "    output = Dense(114, activation='softmax')(combined)\n",
    "\n",
    "\n",
    "    lr = lr\n",
    "    optimizer = optimizer\n",
    "\n",
    "    model = Model(inputs=[image_input, numerical_input, one_hot_input], outputs=output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model(dropout_rate = 0.44015986503365345, num_units = 64, num_units_one_hot = 16, dropout_rate_combined = 0.5989644076124379, optimizer = 'SGD', lr = 0.003068882482985734, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    [X_train_image_augmented, X_train_numerical_augmented, X_train_one_hot_augmented], y_train_augmented,\n",
    "    validation_data=([X_val_image, X_val_numerical, X_val_one_hot], y_val),\n",
    "    epochs=20, batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6f97123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 71s 684ms/step\n",
      "Accuracy: [4.172484397888184, 0.10621920973062515]\n",
      "Test Accuracy: 0.4362684729064039\n",
      "F1 Score: 0.4328489990863967\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict([X_test_image, X_test_numerical, X_test_one_hot])\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
